# 状态与持久化

<cite>
**本文档中引用的文件**  
- [sqlalchemy_workflow_execution_repository.py](file://api/core/repositories/sqlalchemy_workflow_execution_repository.py)
- [sqlalchemy_workflow_node_execution_repository.py](file://api/core/repositories/sqlalchemy_workflow_node_execution_repository.py)
- [workflow_execution.py](file://api/core/workflow/entities/workflow_execution.py)
- [workflow_node_execution.py](file://api/core/workflow/entities/workflow_node_execution.py)
- [workflow.py](file://api/models/workflow.py)
- [workflow_run_fields.py](file://api/fields/workflow_run_fields.py)
- [workflow_execution_tasks.py](file://api/tasks/workflow_execution_tasks.py)
- [workflow_node_execution_tasks.py](file://api/tasks/workflow_node_execution_tasks.py)
</cite>

## 目录
1. [引言](#引言)
2. [工作流执行状态模型](#工作流执行状态模型)
3. [节点执行状态模型](#节点执行状态模型)
4. [执行日志的生成与查询](#执行日志的生成与查询)
5. [状态一致性与事务管理](#状态一致性与事务管理)
6. [执行历史追溯与断点续跑](#执行历史追溯与断点续跑)
7. [审计日志功能实现](#审计日志功能实现)
8. [大规模执行记录的分库分表与索引优化](#大规模执行记录的分库分表与索引优化)

## 引言
Dify 工作流系统通过精细化的状态管理与持久化机制，确保复杂任务执行过程的可追溯性、可靠性和可观测性。本文档深入解析其执行状态的持久化设计，涵盖工作流与节点的执行记录模型、状态流转、日志策略、事务一致性保障及大规模数据处理优化方案。

## 工作流执行状态模型

工作流执行记录（Workflow Execution）在 Dify 中由 `WorkflowExecution` 领域实体和 `WorkflowRun` 数据库模型共同定义。其核心状态字段、时间戳和元数据设计如下：

- **状态字段 (`status`)**: 使用 `WorkflowExecutionStatus` 枚举，包含 `running`（运行中）、`succeeded`（成功）、`failed`（失败）、`stopped`（已停止）和 `partial-succeeded`（部分成功）等状态，精确反映执行生命周期。
- **时间戳**: 包含 `started_at`（开始时间）和 `finished_at`（结束时间）字段。`elapsed_time` 属性通过计算两者差值得出，单位为秒。
- **元数据存储**: 通过 `execution_metadata` 字段以 JSON 格式存储额外的执行上下文信息。
- **输入与输出**: `inputs` 和 `outputs` 字段以 JSON 格式存储执行的输入参数和最终输出结果。
- **性能指标**: 记录 `total_tokens`（总令牌数）和 `total_steps`（总步数），用于成本和性能分析。

该模型通过 `sqlalchemy_workflow_execution_repository` 实现持久化，确保领域模型与数据库模型的双向转换。

**Section sources**
- [workflow_execution.py](file://api/core/workflow/entities/workflow_execution.py#L26-L88)
- [workflow.py](file://api/models/workflow.py#L691-L716)
- [workflow_run_fields.py](file://api/fields/workflow_run_fields.py#L107-L122)

## 节点执行状态模型

节点执行记录（Node Execution）是工作流执行的原子单元，其数据模型设计更为精细：

- **状态字段 (`status`)**: 使用 `WorkflowNodeExecutionStatus` 枚举，包含 `running`（运行中）、`succeeded`（成功）、`failed`（失败）、`exception`（异常）、`retry`（重试）等状态，支持复杂的节点行为。
- **时间戳**: 同样包含 `created_at` 和 `finished_at` 字段，并通过 `elapsed_time` 计算执行耗时。
- **元数据存储**: `execution_metadata` 字段存储节点特有的上下文，如循环（iteration）、并行（parallel）和重试（retry）的标识符（`iteration_id`, `parallel_id`, `parent_parallel_id` 等），用于构建执行日志的结构化视图。
- **执行过程数据**: 除了 `inputs` 和 `outputs`，还包含 `process_data` 字段，用于存储节点在执行过程中的中间状态或处理信息。
- **关联信息**: 明确记录 `workflow_run_id`（关联的工作流执行ID）、`node_id`（节点ID）、`node_type`（节点类型）和 `index`（执行索引）。

该模型由 `sqlalchemy_workflow_node_execution_repository` 管理，实现了与领域实体 `WorkflowNodeExecution` 的映射。

**Section sources**
- [workflow_node_execution.py](file://api/core/workflow/entities/workflow_node_execution.py#L43-L52)
- [sqlalchemy_workflow_node_execution_repository.py](file://api/core/repositories/sqlalchemy_workflow_node_execution_repository.py#L97-L135)
- [workflow.py](file://api/models/workflow.py#L691-L716)

## 执行日志的生成与查询

执行日志的生成是一个由工作流引擎驱动的异步过程：

1.  **生成**: 当工作流或节点状态发生变化时，系统会触发 Celery 任务（如 `workflow_execution_tasks` 和 `workflow_node_execution_tasks`）。这些任务负责将内存中的领域实体（`WorkflowExecution`, `WorkflowNodeExecution`）转换为数据库模型（`WorkflowRun`, `WorkflowNodeExecutionModel`），并将 `status`、`error`、`inputs`、`outputs`、`execution_metadata` 等字段序列化为 JSON 后持久化到数据库。
2.  **查询**: 前端通过 API 接口查询执行记录。后端使用 `sqlalchemy_workflow_execution_repository` 和 `sqlalchemy_workflow_node_execution_repository` 从数据库读取数据，并通过 `workflow_run_fields` 等字段定义将数据库模型反序列化为 API 响应格式。`execution_metadata` 中的结构化信息被用于在 UI 上构建清晰的执行流程图，例如展示循环和并行分支。

**Section sources**
- [workflow_execution_tasks.py](file://api/tasks/workflow_execution_tasks.py#L102-L135)
- [workflow_node_execution_tasks.py](file://api/tasks/workflow_node_execution_tasks.py#L125-L153)
- [sqlalchemy_workflow_execution_repository.py](file://api/core/repositories/sqlalchemy_workflow_execution_repository.py)
- [workflow_run_fields.py](file://api/fields/workflow_run_fields.py)

## 状态一致性与事务管理

Dify 通过数据库事务和领域驱动设计（DDD）来保证状态一致性：

- **数据库事务**: 所有对 `WorkflowRun` 和 `WorkflowNodeExecutionModel` 的写操作（如 `save`）都在数据库会话（session）的事务中执行。这确保了单次状态更新的原子性，防止出现中间状态。
- **领域事件与最终一致性**: 对于跨多个节点的复杂状态变更，系统可能采用事件驱动架构。状态变更首先在本地事务中持久化，然后发布领域事件。后续的处理（如更新父工作流状态）通过监听这些事件异步完成，保证了系统的最终一致性。
- **乐观锁与冲突处理**: 在高并发场景下，通过主键或唯一约束（如 `node_execution_id`）来防止重复插入。代码中通过捕获 `IntegrityError` 异常来处理并发冲突，确保数据的唯一性和完整性。

**Section sources**
- [sqlalchemy_workflow_node_execution_repository.py](file://api/core/repositories/sqlalchemy_workflow_node_execution_repository.py#L162-L189)
- [test_workflow_node_execution_conflict_handling.py](file://api/tests/unit_tests/core/repositories/test_workflow_node_execution_conflict_handling.py#L192-L209)

## 执行历史追溯与断点续跑

- **执行历史追溯**: 由于每次执行的 `inputs`、`outputs`、`status` 和完整的 `execution_metadata` 都被持久化，系统可以完整地回溯任意一次工作流或节点的执行过程。前端通过解析 `execution_metadata` 中的 `iteration_id`、`parallel_id` 等信息，可以可视化地展示复杂的执行路径。
- **断点续跑**: Dify 的设计支持断点续跑。`process_data` 字段存储了节点的中间状态，当节点因临时故障失败并进入 `retry` 状态时，系统可以读取 `process_data` 并从中断点继续执行，而不是从头开始，提高了执行效率和资源利用率。

**Section sources**
- [workflow_node_execution.py](file://api/core/workflow/entities/workflow_node_execution.py)
- [graph-to-log-struct.ts](file://web/app/components/workflow/run/utils/format-log/graph-to-log-struct.ts)
- [common\workflow_response_converter.py](file://api/core/app/apps/common/workflow_response_converter.py#L250-L271)

## 审计日志功能实现

执行记录本身即是一种强大的审计日志。其审计功能体现在：
- **操作溯源**: `created_by_role` 和 `created_by` 字段明确记录了执行的发起者（用户或应用）。
- **完整记录**: 每次执行的输入、输出、状态、错误信息和时间戳都被完整保存，满足审计要求。
- **变更追踪**: 通过查询历史执行记录，可以追踪工作流配置变更对执行结果的影响。

**Section sources**
- [workflow_run_fields.py](file://api/fields/workflow_run_fields.py#L107-L122)
- [workflow.py](file://api/models/workflow.py)

## 大规模执行记录的分库分表与索引优化

为应对海量执行记录，Dify 采用了以下优化策略：
- **分库分表**: 虽然具体实现未在代码中直接体现，但 `WorkflowRun` 和 `WorkflowNodeExecutionModel` 模型设计（如 `tenant_id`, `app_id`, `workflow_id` 作为关键字段）为水平分片（Sharding）提供了基础。可以根据 `tenant_id` 或 `created_at` 进行分库分表，将数据分散到多个数据库实例中。
- **索引优化**: 在数据库层面，对高频查询字段建立索引至关重要。例如，对 `workflow_run_id`（用于查询工作流下的所有节点）、`status`（用于筛选运行中或失败的任务）、`created_at`（用于按时间范围查询）和 `tenant_id`（用于租户隔离查询）建立复合索引，可以极大提升查询性能。
- **数据清理**: 系统通过 Celery 任务（如 `clean_workflow_runlogs_precise.py`）定期清理过期的执行日志，防止数据库无限增长，保障系统性能。

**Section sources**
- [workflow.py](file://api/models/workflow.py#L691-L716)
- [clean_workflow_runlogs_precise.py](file://api/schedule/clean_workflow_runlogs_precise.py)